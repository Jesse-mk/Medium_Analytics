{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = pd.read_csv(\"/Users/jennayang/Documents/Medium Project Data/medium-tech-data.csv\")\n",
    "money = pd.read_csv(\"/Users/jennayang/Documents/Medium Project Data/Medium_Money_Data_final.csv\", index_col = 0) \n",
    "sports = pd.read_csv(\"/Users/jennayang/Documents/Medium Project Data/medium-sports-data.csv\", index_col = 0) \n",
    "politics = pd.read_csv(\"/Users/jennayang/Documents/Medium Project Data/Politics_data_full.csv\", index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there was a bug with the webscraping: people with 2020 claps have the wrong number of claps. \n",
    "#checked via website, having differing number of claps\n",
    "politics = politics[politics.claps != 2020]\n",
    "\n",
    "#get ratio of clap to followers, put as a new feature\n",
    "clap_ratio = politics.claps / politics.followers\n",
    "\n",
    "#get number of followers:\n",
    "followers = politics.followers\n",
    "\n",
    "#put the clap ratio as a new feature\n",
    "politics[\"clap_ratio\"] = clap_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data - removing nulls and empty vals\n",
    "tech = tech[pd.notnull(tech['text'])]\n",
    "tech.reset_index(inplace = True)\n",
    "\n",
    "sports = sports[pd.notnull(sports['text'])]\n",
    "sports.reset_index(inplace = True)\n",
    "\n",
    "politics = politics[pd.notnull(politics['text'])]\n",
    "politics.reset_index(inplace = True)\n",
    "\n",
    "money = money[pd.notnull(money['text'])] \n",
    "money.reset_index(inplace = True)\n",
    "\n",
    "politics = politics[~(politics.text == '[]')]\n",
    "money = money[~(money.text == '[]')]\n",
    "sports = sports[~(sports.text == '[]')]\n",
    "tech = tech[~(tech.text == '[]')]\n",
    "\n",
    "#shuffle and randomly select 2000 entries (downsampling tech and politics) (money keep at 1600) (sports keep at 2500)\n",
    "str1 = tech.sample(2000, replace= False)[[\"text\"]].reset_index(drop=True)\n",
    "str2 = sports.sample(len(sports), replace= False)[[\"text\"]].reset_index(drop=True)\n",
    "str3 = politics.sample(2000, replace= False)[[\"text\"]].reset_index(drop=True)\n",
    "str4 = money.sample(len(money), replace= False)[[\"text\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breaking our data up into training and test sets (80/20 split)\n",
    "pd.options.mode.chained_assignment = None #surpress warning\n",
    "\n",
    "#first, we need to add the labels\n",
    "str1[\"label\"] = [0] * len(str1)\n",
    "str2[\"label\"] = [1] * len(str2)\n",
    "str3[\"label\"] = [2] * len(str3)\n",
    "str4[\"label\"] = [3] * len(str4)\n",
    "\n",
    "\n",
    "#making the training set and test sets\n",
    "str2_break = int(len(str2) * 0.8)\n",
    "str4_break = int(len(str4) * 0.8)\n",
    "str1_train = str1.iloc[:1600] #tech - 2000\n",
    "str2_train = str2.iloc[:str2_break] \n",
    "str3_train = str3.iloc[:1600] #politics - 2000\n",
    "str4_train = str4.iloc[:str4_break]\n",
    "\n",
    "str1_test = str1.iloc[1600:] #tech - 2000\n",
    "str2_test = str2.iloc[str2_break:] \n",
    "str3_test = str3.iloc[1600:] #politics - 2000\n",
    "str4_test = str4.iloc[str4_break:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the combined training sets and test sets\n",
    "one = pd.concat([str1_train, str2_train], axis = 0)\n",
    "two = pd.concat([str3_train, str4_train], axis = 0)\n",
    "train = pd.concat([one, two], axis = 0)\n",
    "\n",
    "first = pd.concat([str1_test, str2_test], axis = 0)\n",
    "second = pd.concat([str3_test, str4_test], axis = 0)\n",
    "test = pd.concat([first, second], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling\n",
    "train = train.sample(frac = 1, replace = False).reset_index(drop=True)\n",
    "test = test.sample(frac = 1, replace = False).reset_index(drop=True)\n",
    "\n",
    "#get X and Y data from the sets\n",
    "all_words = train.text\n",
    "Y_training = train.label\n",
    "\n",
    "all_words_test = test.text\n",
    "Y_test = test.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The story is part of the SF Homeless Project, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>['[This is especially true as you get closer t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I was going to lead off this post with Miguel ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>['[I love the New York Times. Or loved. I canc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Three Saturdays ago, a 19-year-old man posted ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The story is part of the SF Homeless Project, ...      0\n",
       "1  ['[This is especially true as you get closer t...      3\n",
       "2  I was going to lead off this post with Miguel ...      1\n",
       "3  ['[I love the New York Times. Or loved. I canc...      2\n",
       "4  Three Saturdays ago, a 19-year-old man posted ...      0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', 'american', 'best', 'better', 'day', 'did', 'does', 'dont', 'financial', 'game', 'going', 'good', 'government', 'im', 'income', 'just', 'know', 'life', 'like', 'long', 'make', 'market', 'money', 'need', 'new', 'pay', 'people', 'point', 'political', 'president', 'really', 'right', 'said', 'say', 'state', 'states', 'team', 'thats', 'things', 'think', 'time', 'trump', 'use', 'want', 'way', 'work', 'world', 'year', 'years', 'youre']\n"
     ]
    }
   ],
   "source": [
    "#create the vector\n",
    "#focus on max_features as well\n",
    "vector = tfidf(stop_words = \"english\", strip_accents = 'ascii', max_features = 50)\n",
    "\n",
    "#fit the data\n",
    "vector.fit(all_words)\n",
    "\n",
    "#can look at the vectorizer\n",
    "print(vector.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the data, manipulating it based on the weights we \n",
    "#determined before by fitting our data \n",
    "train_transformed = vector.transform(all_words)\n",
    "test_transformed = vector.transform(all_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6537x50 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 88969 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<4902x50 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 67469 stored elements in Compressed Sparse Row format>,\n",
       " <1635x50 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 22910 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting\n",
    "train_test_split(train_transformed, Y_training)[0], train_test_split(train_transformed, Y_training)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the KNN model\n",
    "knearest = knn(n_neighbors = 10)\n",
    "\n",
    "#fit the model with training data\n",
    "knearest.fit(train_transformed, Y_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314984709480122"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knearest.score(test_transformed, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying different k values\n",
    "knn_scores = []\n",
    "\n",
    "for i in range(30):\n",
    "    knearest = knn(n_neighbors = i+1)\n",
    "    knearest.fit(train_transformed, Y_training)\n",
    "    \n",
    "    knn_scores.append(knearest.score(test_transformed, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_scores.index(max(knn_scores))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNearestNeighbors Notes\n",
    "\n",
    "unsuprevised, so we don't know the labels (talk about this fact)\n",
    "try different values of k\n",
    "\n",
    "\n",
    "\n",
    "Make a model called Kmeans from sklearn.cluster import kmeans\n",
    "\n",
    "Inertia of the model: how close the clusters are to one another, how clustered the clusters are\n",
    "\n",
    "Make n clusters = k, increase the k value from 0 to certain #, fit X data, append to list the inertia\n",
    "\n",
    "look up: elbow method for optimal value of k in kmeans\n",
    "\n",
    "\n",
    "KNN wouldn't be the best method for us because we do have the labels, so we can just use supervised.\n",
    "\n",
    "keep in mind: keep max features low b/c curse of dimensionality\n",
    "\n",
    "\n",
    "Graphs: graph of the k's (elbow).\n",
    "\n",
    "Analysis: compare distortions vs inertia, analyze the clusters, what's inside, which ones tend to get clustered together or not, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
