{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting Together Webscraping HTMLs and parsing through the page\n",
    "by Harsha and Jesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup and requests library import\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned htmls (entire html should work)\n",
    "#use hte \"final_topic\".csv for this part\n",
    "htmls = pd.read_csv(\"final_politics.csv\", index_col = 0, header = None).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list from the dataframe with htmls\n",
    "h = htmls[1].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dictionary to fill with keys: urls, data: data from article\n",
    "htmls_with_data = {}\n",
    "\n",
    "htmls_with_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "1\n",
      "<Response [200]>\n",
      "2\n",
      "<Response [200]>\n",
      "3\n",
      "<Response [200]>\n",
      "4\n",
      "<Response [200]>\n",
      "5\n",
      "<Response [200]>\n",
      "6\n",
      "<Response [200]>\n",
      "7\n",
      "<Response [200]>\n",
      "8\n",
      "<Response [200]>\n",
      "9\n",
      "<Response [200]>\n",
      "10\n",
      "<Response [200]>\n",
      "11\n",
      "<Response [200]>\n",
      "12\n",
      "<Response [200]>\n",
      "13\n",
      "<Response [200]>\n",
      "14\n",
      "<Response [200]>\n",
      "15\n",
      "<Response [200]>\n",
      "16\n",
      "<Response [200]>\n",
      "17\n",
      "<Response [200]>\n",
      "18\n",
      "<Response [200]>\n",
      "19\n",
      "<Response [200]>\n",
      "20\n",
      "<Response [200]>\n",
      "21\n",
      "<Response [200]>\n",
      "22\n",
      "<Response [200]>\n",
      "23\n",
      "<Response [200]>\n",
      "24\n",
      "<Response [200]>\n",
      "25\n",
      "<Response [200]>\n",
      "26\n",
      "<Response [200]>\n",
      "27\n",
      "<Response [200]>\n",
      "28\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#making for loop to scrape the data:\n",
    "counter = 1\n",
    "\n",
    "for i in h:\n",
    "    time.sleep(5)\n",
    "    #page request\n",
    "    page_response = requests.get(i, timeout=5)\n",
    "    print(page_response)\n",
    "    \n",
    "    #parsing content from page and storing it\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    \n",
    "    #empty list to store user data\n",
    "    data = []\n",
    "    \n",
    "    #try to grab header\n",
    "    try:\n",
    "        #grabs div that stores title info\n",
    "        titleInfo = page_content.find_all('div',attrs={\"class\":\"n p\"})\n",
    "\n",
    "        #finds title within titleInfo div\n",
    "        href = str(titleInfo).index(\"<h1\")\n",
    "        start = str(titleInfo)[href:].index(\">\")+href+1\n",
    "        end = str(titleInfo)[href:].index(\"</h1>\")+href\n",
    "\n",
    "        #appends title to data list after removing extra tags\n",
    "        title = re.sub(re.compile('<.*?>'), '', str(titleInfo)[start:end])\n",
    "        data.append(title)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    #try to grab author\n",
    "    try:\n",
    "\n",
    "        #<h1>Finding and Adding Author to Data</h1>\n",
    "\n",
    "        #grabs div that stores user info\n",
    "        userInfo = page_content.find_all('div',attrs={\"class\":\"o n\"})\n",
    "\n",
    "        #finds authors name within userInfo div\n",
    "        href = str(userInfo)[24:].index(\"<a class=\")+24\n",
    "        start = str(userInfo)[href:].index(\">\")+href+1\n",
    "        end = str(userInfo)[href:].index(\"</a>\")+href\n",
    "\n",
    "        #appends authors name to data list\n",
    "        author = str(userInfo)[start:end]\n",
    "        data.append(author)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    try:\n",
    "        #trying to get author page url:\n",
    "\n",
    "        #<h1>Finding and Adding Author Page URL to Data</h1>\n",
    "\n",
    "        #finds date within userInfo div\n",
    "        href = str(userInfo)[24:].index(\"<a class=\")\n",
    "        start = str(userInfo)[href:].index(\"href=\")+href+6\n",
    "        end = str(userInfo)[start:].index(\"?source\")+start\n",
    "\n",
    "        #appends date to data list\n",
    "        url = \"medium.com\" + str(userInfo)[start:end]\n",
    "        data.append(url)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    ### PARSING THROUGH USE PAGE NOW ###\n",
    "    try: #adding user data:\n",
    "        #user url\n",
    "        user_link = \"https://\" + data[2]\n",
    "        #user page request\n",
    "        time.sleep(5)\n",
    "        user_response = requests.get(user_link, timeout=5)\n",
    "        #parsing content from user page to store follower count\n",
    "        user_content = BeautifulSoup(user_response.content, \"html.parser\")\n",
    "        #user tag\n",
    "        user_href = re.sub(\"medium.com\", \"\", data[2])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try: #get member since\n",
    "        memberInfo = user_content.find_all(\"span\")\n",
    "        year = re.findall(\"[0-9]{4}\", str(memberInfo))[0]\n",
    "        data.append(year)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    \n",
    "    try: #get following:\n",
    "        #returns following count and adds to data list\n",
    "        followingInfo = user_content.find_all('a', attrs={\"href\":user_href+\"/following\"})\n",
    "        following = re.sub(\"[^0-9]\", \"\", str(followingInfo))\n",
    "        data.append(following)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #get follower count\n",
    "    try: \n",
    "        #returns follower count and adds to data list\n",
    "        followerInfo = user_content.find_all('a', attrs={\"href\":user_href+\"/followers\"})\n",
    "        followers = re.sub(\"[^0-9]\", \"\", str(followerInfo))\n",
    "        data.append(followers)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #trying to add date\n",
    "    try:\n",
    "        #<h1>Finding and Adding Date to Data</h1>\n",
    "\n",
    "        #finds date within userInfo div\n",
    "        href = str(userInfo)[end:].index(\"<a class=\")+end\n",
    "        start = str(userInfo)[href:].index(\">\")+href+1\n",
    "        end = str(userInfo)[href:].index(\"</a>\")+href\n",
    "\n",
    "        #appends date to data list\n",
    "        date = str(userInfo)[start:end]\n",
    "        data.append(date)\n",
    "    except:\n",
    "        data.append(None)\n",
    "\n",
    "    #tring to add clap number\n",
    "    try:\n",
    "        #<h1>Finding and Adding Number of Claps to Data</h1>\n",
    "\n",
    "        #grabs all paragraphs in the article\n",
    "        clapInfo = page_content.find_all('div',attrs={\"class\":\"n o\"})\n",
    "\n",
    "        #finds claps within clapInfo div\n",
    "        result = re.sub(re.compile('<.*?>'), '', str(clapInfo))\n",
    "        result = re.findall(\"\\d+\", result)\n",
    "\n",
    "        #appends claps to data list\n",
    "        if len(result) == 0:\n",
    "            data.append('0')\n",
    "\n",
    "        else:\n",
    "            data.append(result[0])\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #getting all the text data:\n",
    "    try:\n",
    "        \n",
    "        #<h1>Adding a List of the Text Content to Data</h1>\n",
    "\n",
    "        #grabs all paragraphs in the article\n",
    "        textContent = page_content.find_all(\"p\")\n",
    "        textInfo = str(textContent)\n",
    "\n",
    "        #RegEx that cleans paragraph data by removing html tags and extra commas\n",
    "        result = re.sub(re.compile('<.*?>'), '', textInfo)\n",
    "        result = re.split(\"\\., \", result)\n",
    "        result = result[:-1]\n",
    "        data.append(result)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    \n",
    "    #append html to dictionary with data\n",
    "    htmls_with_data[i] = data\n",
    "\n",
    "    \n",
    "    #counter\n",
    "    print(counter)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn dictionary into a df\n",
    "dictionary_of_data = pd.DataFrame(htmls_with_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 519)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_of_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot to make the index the url and resetting to get it\n",
    "full_data = dictionary_of_data.T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the urls to be something easier to interpret\n",
    "full_data.columns = [\"url\", \"title\", \"author\", \"username\", \"published\", \"claps\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to csv\n",
    "full_data.to_csv(\"Medium_Politics_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
