{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting Together Webscraping HTMLs and parsing through the page\n",
    "by Harsha and Jesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup and requests library import\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned htmls (entire html should work)\n",
    "htmls = pd.read_csv(\"cleaned_htmls.csv\", index_col = 0, header = None).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs http before else will not work\n",
    "htmls = \"http://\" + htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list from the dataframe with htmls\n",
    "h = htmls[1].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary to fill with keys: urls, data: data from article\n",
    "htmls_with_data = {}\n",
    "\n",
    "htmls_with_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after 50 articles the 3 second sleep ran into an error so using 5 second sleep instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making for loop to scrape the data:\n",
    "counter = 1\n",
    "\n",
    "for i in h[53:]:\n",
    "    time.sleep(5)\n",
    "    #page request\n",
    "    page_response = requests.get(i, timeout=5)\n",
    "    print(page_response)\n",
    "    \n",
    "    #parsing content from page and storing it\n",
    "    page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    \n",
    "    #empty list to store user data\n",
    "    data = []\n",
    "    \n",
    "    try:\n",
    "        #grabs div that stores title info\n",
    "        titleInfo = page_content.find_all('div',attrs={\"class\":\"n p\"})\n",
    "\n",
    "        #finds title within titleInfo div\n",
    "        href = str(titleInfo).index(\"<h1\")\n",
    "        start = str(titleInfo)[href:].index(\">\")+href+1\n",
    "        end = str(titleInfo)[href:].index(\"</h1>\")+href\n",
    "\n",
    "        #appends title to data list after removing extra tags\n",
    "        title = re.sub(re.compile('<.*?>'), '', str(titleInfo)[start:end])\n",
    "        data.append(title)\n",
    "\n",
    "        #<h1>Finding and Adding Author to Data</h1>\n",
    "\n",
    "        #grabs div that stores user info\n",
    "        userInfo = page_content.find_all('div',attrs={\"class\":\"o n\"})\n",
    "\n",
    "        #finds authors name within userInfo div\n",
    "        href = str(userInfo)[24:].index(\"<a class=\")+24\n",
    "        start = str(userInfo)[href:].index(\">\")+href+1\n",
    "        end = str(userInfo)[href:].index(\"</a>\")+href\n",
    "\n",
    "        #appends authors name to data list\n",
    "        author = str(userInfo)[start:end]\n",
    "        data.append(author)\n",
    "\n",
    "        #<h1>Finding and Adding Author Page URL to Data</h1>\n",
    "\n",
    "        #finds date within userInfo div\n",
    "        href = str(userInfo)[24:].index(\"<a class=\")\n",
    "        start = str(userInfo)[href:].index(\"href=\")+href+6\n",
    "        end = str(userInfo)[start:].index(\"?source\")+start\n",
    "\n",
    "        #appends date to data list\n",
    "        url = \"medium.com\" + str(userInfo)[start:end]\n",
    "        data.append(url)\n",
    "\n",
    "        #<h1>Finding and Adding Date to Data</h1>\n",
    "\n",
    "        #finds date within userInfo div\n",
    "        href = str(userInfo)[end:].index(\"<a class=\")+end\n",
    "        start = str(userInfo)[href:].index(\">\")+href+1\n",
    "        end = str(userInfo)[href:].index(\"</a>\")+href\n",
    "\n",
    "        #appends date to data list\n",
    "        date = str(userInfo)[start:end]\n",
    "        data.append(date)\n",
    "\n",
    "        #<h1>Finding and Adding Number of Claps to Data</h1>\n",
    "\n",
    "        #grabs all paragraphs in the article\n",
    "        clapInfo = page_content.find_all('div',attrs={\"class\":\"n o\"})\n",
    "\n",
    "        #finds claps within clapInfo div\n",
    "        result = re.sub(re.compile('<.*?>'), '', str(clapInfo))\n",
    "        result = re.findall(\"\\d+\", result)\n",
    "\n",
    "        #appends claps to data list\n",
    "        if len(result) == 0:\n",
    "            data.append('0')\n",
    "\n",
    "        else:\n",
    "            data.append(result[0])\n",
    "\n",
    "        #<h1>Adding a List of the Text Content to Data</h1>\n",
    "\n",
    "        #grabs all paragraphs in the article\n",
    "        textContent = page_content.find_all(\"p\")\n",
    "        textInfo = str(textContent)\n",
    "\n",
    "        #RegEx that cleans paragraph data by removing html tags and extra commas\n",
    "        result = re.sub(re.compile('<.*?>'), '', textInfo)\n",
    "        result = re.split(\"\\., \", result)\n",
    "        result = result[:-1]\n",
    "        data.append(result)\n",
    "    \n",
    "        #append html to dictionary with data\n",
    "        htmls_with_data[i] = data\n",
    "    except:\n",
    "        htmls_with_data[i] = None\n",
    "    \n",
    "    #counter\n",
    "    print(counter)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn dictionary into a df\n",
    "dictionary_of_data = pd.DataFrame(htmls_with_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot to make the index the url and resetting to get it\n",
    "full_data = dictionary_of_data.T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the urls to be something easier to interpret\n",
    "full_data.columns = [\"url\", \"title\", \"author\", \"username\", \"published\", \"claps\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to csv\n",
    "full_data.to_csv(\"Medium_Articles_Art_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
