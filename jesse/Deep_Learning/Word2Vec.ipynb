{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec \n",
    "word embedding using Stanford's word embedding tool (Google's Word2Vec is too slow and memory consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = \"./embedding/GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### STANFORD WORD2VEC ###\n",
    "#create word2vec model:\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "####SINCE ALREADY CREATED EMBEDDING FILE, DON'T NEED TO RUN CODE BELOW ###\n",
    "\n",
    "#glove_input_file = './embedding/glove.6B.50d.txt'\n",
    "#word2vec_output_file = './embedding/word2vec.txt'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word2vec model: \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('./embedding/word2vec.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings\n",
    "# import weights from embedder into torch\n",
    "weights = torch.FloatTensor(model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "money = pd.read_csv(\"./final_csvs/money.csv\")\n",
    "text = money[\"text\"].str.strip(\"['[\").str.strip(\"']\") #.str.strip(\"By w+,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive = ['woman', 'king'], negative = ['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model using PyTorch tutorial by Robert Guthrie at\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "money = pd.read_csv(\"./final_csvs/money.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = money[\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize LSTM:\n",
    "lstm = nn.LSTM(2,2) #input 2, output 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any embedding, need to get a dictionary of words that have values (index/position and embedding vector). Actually gensim does this for us automatically. example below: just use \"model\" as a dictionary and you get an embedding back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.047361, -0.61568 , -0.46639 ,  0.12377 ,  0.82296 ,  0.4487  ,\n",
       "       -0.50167 , -0.38806 ,  0.17726 ,  0.4479  ,  0.58177 ,  0.40022 ,\n",
       "        1.0288  ,  0.48169 ,  0.014657,  0.12539 , -0.48038 ,  0.64939 ,\n",
       "       -0.19735 , -1.0002  , -0.17713 , -0.51028 ,  1.359   , -0.026779,\n",
       "       -0.29515 ,  0.22049 , -1.2501  ,  0.57992 ,  0.6166  , -0.67485 ,\n",
       "        0.48102 , -0.45007 , -0.86688 ,  1.614   ,  0.13927 , -0.17601 ,\n",
       "       -0.51826 ,  0.14967 ,  1.3347  ,  0.18456 ,  0.5102  ,  0.2141  ,\n",
       "       -0.84844 , -0.35747 ,  0.703   ,  0.94865 ,  0.11861 , -0.50851 ,\n",
       "       -0.89103 , -0.14026 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"jelly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "Used above link as starter code to try and make RNN\n",
    "\n",
    "right now we have a dictionary, need to create an embedding layer. need a matrix of weights for embedding layer. shape = (dataset vocab length, word vec dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#get all the text into one vector... might take a little while.\n",
    "\n",
    "full_text = text.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_words = set(pd.Series(full_text.split(\" \")).str.strip(\",\").str.strip(\".\").str.strip(\" \").str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['[Potential ways to Save using a Free Library Card', 'Last year, I was contemplating to buy a premium subscription from one of the well-known investment research journals to improve my investment IQ.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing alphanumeric:  https://stackoverflow.com/questions/1276764/stripping-everything-but-alphanumeric-chars-from-a-string-in-python\n",
    "\n",
    "#get rid of unnecessary punctuation\n",
    "pattern = re.compile(\"[^\\w\\.\\!\\?-]+\")\n",
    "words = pattern.sub(\" \", full_text)\n",
    "\n",
    "#keep ending punctuation\n",
    "pattern = re.compile(\"[\\.]+\")\n",
    "periods = pattern.sub(\" .\", words)\n",
    "\n",
    "pattern = re.compile(\"[\\!]+\")\n",
    "exclaim = pattern.sub(\" !\", periods)\n",
    "\n",
    "pattern = re.compile(\"[\\?]+\")\n",
    "questions = pattern.sub(\" ?\", exclaim)\n",
    "\n",
    "all_words = questions.lower().strip().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make a matrix with the length of all the words in the vocab\n",
    "len_matrix = len(article_words)\n",
    "\n",
    "#should be a matrix of vocab across, word vector dimension down\n",
    "weight_matrix = np.zeros((len_matrix, 50))\n",
    "\n",
    "#words not found in Glove\n",
    "words_not_found = 0\n",
    "weird_words = []\n",
    "\n",
    "#make a matrix\n",
    "for i, word in enumerate(article_words):\n",
    "    try: \n",
    "        weight_matrix[i] = model[word]\n",
    "    except:\n",
    "        weight_matrix[i] = np.random.normal(scale =0.6, size = (50,))\n",
    "        words_not_found +=1\n",
    "        weird_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The amount of words not found is equal to 6533 which is 16.98%'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seems like there are a few words which aren't present in the encoding\n",
    "\"The amount of words not found is equal to {} which is {}%\".format(words_not_found, round(words_not_found/len_matrix*100, 2))\n",
    "\n",
    "#weird_words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a NN with embedding layer = first layer. Needs to make the words quantitative (ie make them into vectors like one-hot encoding but with embedding rather than binary values.) Transforms the original input words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_in_tensor = torch.from_numpy(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(weights, train=True):\n",
    "    \"\"\"Takes in a numpy array of weights as created by above and creates\n",
    "    an embedding layer. Returns layer and dimensions\"\"\"\n",
    "    \n",
    "    #pytorch needs tensors:\n",
    "    weights = torch.from_numpy(weights)\n",
    "    \n",
    "    #get dimensions of weight vector, words vs dimension vector\n",
    "    embed_num, embed_dim = weights.shape[0], weights.shape[1]\n",
    "    \n",
    "    #the embedding layer needs to be this dimension of weights matrix\n",
    "    embed_layer = nn.Embedding(embed_num, embed_dim)\n",
    "    \n",
    "    #load in the dictionary of weights from the embedder\n",
    "    embed_layer.load_state_dict({\"weight\": weights})\n",
    "    \n",
    "    #updates conditions if the layers should be trained or not\n",
    "    if train:\n",
    "        embed_layer.weight.requires_grad = True\n",
    "    \n",
    "    #return the layer with the embedding, and its dimensions\n",
    "    return embed_layer, embed_num, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(38476, 50), 38476, 50)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(weight_matrix, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the RNN: based on the code in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"Making a NN LSTM model with embedding layer\"\"\"\n",
    "    def __init__(self, weights, hidden_layer_size, num_layers):\n",
    "        #use the super class (nn.Module) initializer\n",
    "        super(self).__init__()\n",
    "        self.embedding, embed_num, embed_dim = embedding_layer(weight_matrix)\n",
    "        self.hidden_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_layer_size, num_layers, batch_first = True)\n",
    "    \n",
    "    def forward(self, input_values, hidden):\n",
    "        return self.lstm(self.embedding(input_values), hidden)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self,nunm_laeyrs, batch_size, self.hidden_size))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super() argument 1 must be type, not NN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-dafb237168f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-157-5624583815ab>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, weights, hidden_layer_size, num_layers)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m#use the super class (nn.Module) initializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super() argument 1 must be type, not NN"
     ]
    }
   ],
   "source": [
    "lstm = NN(weight_matrix, 100, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
