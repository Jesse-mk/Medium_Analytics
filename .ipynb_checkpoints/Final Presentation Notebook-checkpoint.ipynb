{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Medium Analytics\n",
    "##### by  Harsha, Jenna, Shinu, and Jesse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 0: Check the robots.txt\n",
    "to see what you can scrape at: https://medium.com/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Part 1: Sample - looking at a single page\n",
    "basic webpage accessing and scraping the top of the art topics page (without scrolling). Only about 10 articles. Was able to retrieve links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#the url topic (root url)\n",
    "url = \"https://medium.com/topic/art\"\n",
    "\n",
    "#requesting the url to get access to the page\n",
    "r = requests.get(url)\n",
    "#should be 200\n",
    "print(r)\n",
    "\n",
    "#parsing in the information\n",
    "soup = BeautifulSoup(r.content, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <script>\n",
      "   !function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a=\"pointerup\",u=\"pointercancel\";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;\"pointerdown\"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)\n"
     ]
    }
   ],
   "source": [
    "#look at the html in nice format\n",
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/@Rashmee/questions-still-linger-about-colonial-era-artefacts-18876e0e824?source=topic_page---------0------------------1',\n",
       " '/@Rashmee/questions-still-linger-about-colonial-era-artefacts-18876e0e824?source=topic_page---------0------------------1',\n",
       " '/@Rashmee/questions-still-linger-about-colonial-era-artefacts-18876e0e824?source=topic_page---------0------------------1',\n",
       " '/@Rashmee?source=topic_page---------0------------------1',\n",
       " '/@Rashmee/questions-still-linger-about-colonial-era-artefacts-18876e0e824?source=topic_page---------0------------------1',\n",
       " '/@chrisjones_32882/reading-art-paul-klees-twittering-machine-e36b88609c58?source=topic_page---------1------------------1',\n",
       " '/@chrisjones_32882/reading-art-paul-klees-twittering-machine-e36b88609c58?source=topic_page---------1------------------1',\n",
       " '/@chrisjones_32882/reading-art-paul-klees-twittering-machine-e36b88609c58?source=topic_page---------1------------------1',\n",
       " '/@chrisjones_32882?source=topic_page---------1------------------1',\n",
       " '/@chrisjones_32882/reading-art-paul-klees-twittering-machine-e36b88609c58?source=topic_page---------1------------------1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REGEX ###\n",
    "#example of link I want to get\n",
    "'<a href=\"https://psiloveyou.xyz/remembering-the-terrible-cb7ebf24a6da?source=topic_page---------6------------------1\"'\n",
    "\n",
    "#Regex pattern to get html tags (want entire html)\n",
    "#divs and labels were too confusing so this was the best/easiest way to get urls\n",
    "pattern = 'href=\"(.{5,100}source=topic_page\\-+\\d\\-+\\d)'\n",
    "\n",
    "#find all the htmls in the string using REGEX\n",
    "result = re.findall(pattern, str(soup))\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Using Selenium to Scroll and Get all the URLs at once \n",
    "creating a new browser that will automatically scroll down the page for you. Using the developer tools it was noticed that \"POST\" calls were being made when the scroll down occurred, but no \"GET\" calls were made. Thus, since we could not use the GET calls, we decided to use the scroller instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#opens a new browser to scroll down automatically\n",
    "browser = webdriver.Chrome(executable_path = r\"C:\\Users\\jesse\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "browser.get(\"https://medium.com/topic/politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#code based off of stackoverflow code #4\n",
    "time.sleep(1)\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "pagedowns = 100000\n",
    "while pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    pagedowns-=1\n",
    "    \n",
    "pages = browser.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Processing\n",
    "using the regex tested above, able to get html links from the browser object. Then we dropped duplicates and cleaned the links. Some of the processing is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://gen.medium.com/does-cutting-u-s-aid-help-or-hurt-central-america-55db640f2add?source=topic_page---------5109------------------1',\n",
       " 'https://gen.medium.com/does-cutting-u-s-aid-help-or-hurt-central-america-55db640f2add?source=topic_page---------5109------------------1',\n",
       " '/@johnbwashington?source=topic_page---------5109------------------1',\n",
       " 'https://gen.medium.com/?source=topic_page---------5109------------------1',\n",
       " 'https://gen.medium.com/does-cutting-u-s-aid-help-or-hurt-central-america-55db640f2add?source=topic_page---------5109------------------1']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regex pattern to get html tags\n",
    "pattern = 'href=\"(.{5,100}source=topic_page\\-+\\d{1,5}\\-+\\d)'\n",
    "\n",
    "#get all the html links based on the pattern\n",
    "result = re.findall(pattern, pages)\n",
    "\n",
    "#put into a series to process\n",
    "html_links = pd.Series(result)\n",
    "\n",
    "#export to csv for safekeeping\n",
    "html_links.to_csv(\"htmls_politics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#using the saved csv\n",
    "topic = pd.read_csv(\"htmls_politics.csv\", index_col = 0, header = None)\n",
    "\n",
    "#drop all the duplicates\n",
    "htmls = topic.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://arcdigital.media/trumps-trade-war-is-k...\n",
       "1    /@maxburnswrites?source=topic_page---------0--...\n",
       "2    https://arcdigital.media/?source=topic_page---...\n",
       "3    /@fnfwriter?source=topic_page---------1-------...\n",
       "4    /politically-speaking?source=topic_page-------...\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a look at some of the htmls... seems as though some are missing the \"https:\"\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#just get the url column\n",
    "htmls = htmls[1]\n",
    "\n",
    "#there are some repeat html links, so getting rid of the repeats\n",
    "clean_htmls = htmls[~htmls.str.contains(r\"^.{2,30}\\?source=topic\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "### ADD HTTP to some versus not others ###\n",
    "\n",
    "#links with https already included (no user in url)\n",
    "with_http = clean_htmls[clean_htmls.str.contains(\"https://\")].reset_index(drop=True)\n",
    "\n",
    "#links without https:\n",
    "without_http = clean_htmls[~clean_htmls.str.contains(\"https://\")].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#adding medium before in order to get the full url\n",
    "urls = \"https://medium.com\" + without_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1679    https://magenta.as/legendary-cartoonist-ben-ka...\n",
       "1680    https://timeline.com/hannah-wilke-labial-art-9...\n",
       "1681    https://artplusmarketing.com/kathy-griffins-ar...\n",
       "1682    https://medium.muz.li/why-gradients-are-the-ne...\n",
       "1683    https://brightthemag.com/a-tale-of-two-artists...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine all the urls together\n",
    "pd.concat([urls, with_http], ignore_index = True).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#cleaned htmls (entire html should work)\n",
    "urls.to_csv(\"cleaned_htmls.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Webscraping Individual Articles\n",
    "the next slide will be a snippet of code used to get the data from the article. We saved it to a dictionary, pivoted it, and then converted it to a dataframe and exported it to a csv. This was done for topics Art, Money, Politics, Sports, and Tech to get information on [\"url\", \"title\", \"author\", \"username\", \"user_since\", \"following\", \"followers\", \"published\", \"claps\", \"text\", \"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#create a dictionary to fill with keys: urls, data: data from article\n",
    "htmls_with_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#making for loop to scrape the data:\n",
    "#starts out pinging every second then increases if \n",
    "counter = 1\n",
    "sleep_amt = 1\n",
    "sleep_amt_2 = 1\n",
    "\n",
    "for i in h:\n",
    "    time.sleep(sleep_amt)\n",
    "    try:\n",
    "        #page request\n",
    "        page_response = requests.get(i, timeout=10)\n",
    "\n",
    "        #parsing content from page and storing it\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "        #empty list to store user data\n",
    "        data = []\n",
    "    except:\n",
    "        sleep_amt = sleep_amt + 1\n",
    "        print(\"new sleep amt is \" + str(sleep_amt))\n",
    "        time.sleep(10)\n",
    "        page_response = requests.get(i, timeout=10)\n",
    "        print(page_response)\n",
    "\n",
    "        #parsing content from page and storing it\n",
    "        page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "\n",
    "        #empty list to store user data\n",
    "        data = []\n",
    "    try:\n",
    "        #grabs div that stores title info\n",
    "        titleInfo = page_content.find_all('div',attrs={\"class\":\"n p\"})\n",
    "\n",
    "        #finds title within titleInfo div\n",
    "        href = str(titleInfo).index(\"<h1\")\n",
    "        start = str(titleInfo)[href:].index(\">\")+href+1\n",
    "        end = str(titleInfo)[href:].index(\"</h1>\")+href\n",
    "\n",
    "        #appends title to data list after removing extra tags\n",
    "        title = re.sub(re.compile('<.*?>'), '', str(titleInfo)[start:end])\n",
    "        data.append(title)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    #try to grab author\n",
    "    try:\n",
    "\n",
    "        #<h1>Finding and Adding Author to Data</h1>\n",
    "\n",
    "        #grabs div that stores user info\n",
    "        userInfo = page_content.find_all('div',attrs={\"class\":\"o n\"})\n",
    "\n",
    "        #finds authors name within userInfo div\n",
    "        href = str(userInfo)[24:].index(\"<a class=\")+24\n",
    "        start = str(userInfo)[href:].index(\">\")+href+1\n",
    "        end = str(userInfo)[href:].index(\"</a>\")+href\n",
    "\n",
    "        #appends authors name to data list\n",
    "        author = str(userInfo)[start:end]\n",
    "        data.append(author)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    try:\n",
    "        #trying to get author page url:\n",
    "\n",
    "        #<h1>Finding and Adding Author Page URL to Data</h1>\n",
    "\n",
    "        #finds date within userInfo div\n",
    "        href = str(userInfo)[24:].index(\"<a class=\")\n",
    "        start = str(userInfo)[href:].index(\"href=\")+href+6\n",
    "        end = str(userInfo)[start:].index(\"?source\")+start\n",
    "\n",
    "        #appends date to data list\n",
    "        url = \"medium.com\" + str(userInfo)[start:end]\n",
    "        data.append(url)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    ### PARSING THROUGH USER PAGE NOW ###\n",
    "    try: #adding user data:\n",
    "        #user url\n",
    "        user_link = \"https://\" + data[2]\n",
    "        #user page request\n",
    "        time.sleep(sleep_amt_2)\n",
    "        user_response = requests.get(user_link, timeout=5)\n",
    "        #parsing content from user page to store follower count\n",
    "        user_content = BeautifulSoup(user_response.content, \"html.parser\")\n",
    "        #user tag\n",
    "        user_href = re.sub(\"medium.com\", \"\", data[2])\n",
    "    except:\n",
    "        response_code = re.findall(\"\\d\\d\\d\",str(user_response))[0]\n",
    "        print(\"error getting user_data\" + str(response_code))\n",
    "        print(user_link)\n",
    "        if response_code != \"429\":\n",
    "            pass\n",
    "        else:\n",
    "            #add onto sleep\n",
    "            sleep_amt_2 = sleep_amt_2 + 1\n",
    "            print(\"new sleep amt 2 is \" + str(sleep_amt_2))\n",
    "            time.sleep(10)\n",
    "        \n",
    "            #adding user data:\n",
    "            #user url\n",
    "            user_link = \"https://\" + data[2]\n",
    "            #user page request\n",
    "\n",
    "            user_response = requests.get(user_link, timeout=5)\n",
    "            #parsing content from user page to store follower count\n",
    "            user_content = BeautifulSoup(user_response.content, \"html.parser\")\n",
    "            #user tag\n",
    "            user_href = re.sub(\"medium.com\", \"\", data[2])\n",
    "\n",
    "    try: #get member since\n",
    "        memberInfo = user_content.find_all(\"span\")\n",
    "        year = re.findall(\"[0-9]{4}\", str(memberInfo))[0]\n",
    "        data.append(year)\n",
    "    except:\n",
    "        data.append(None)\n",
    "    \n",
    "    try: #get following:\n",
    "        #returns following count and adds to data list\n",
    "        followingInfo = user_content.find_all('a', attrs={\"href\":user_href+\"/following\"})\n",
    "        following = re.sub(\"[^0-9]\", \"\", str(followingInfo))\n",
    "        data.append(following)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #get follower count\n",
    "    try: \n",
    "        #returns follower count and adds to data list\n",
    "        followerInfo = user_content.find_all('a', attrs={\"href\":user_href+\"/followers\"})\n",
    "        followers = re.sub(\"[^0-9]\", \"\", str(followerInfo))\n",
    "        data.append(followers)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #trying to add date\n",
    "    try:\n",
    "        #<h1>Finding and Adding Date to Data</h1>\n",
    "\n",
    "        #finds date within userInfo div\n",
    "        href = str(userInfo)[end:].index(\"<a class=\")+end\n",
    "        start = str(userInfo)[href:].index(\">\")+href+1\n",
    "        end = str(userInfo)[href:].index(\"</a>\")+href\n",
    "\n",
    "        #appends date to data list\n",
    "        date = str(userInfo)[start:end]\n",
    "        data.append(date)\n",
    "    except:\n",
    "        data.append(None)\n",
    "\n",
    "    #tring to add clap number\n",
    "    try:\n",
    "        #<h1>Finding and Adding Number of Claps to Data</h1>\n",
    "\n",
    "        #grabs all paragraphs in the article\n",
    "        clapInfo = page_content.find_all('div',attrs={\"class\":\"n o\"})\n",
    "\n",
    "        #finds claps within clapInfo div\n",
    "        result = re.sub(re.compile('<.*?>'), '', str(clapInfo))\n",
    "        result = re.findall(\"\\d+\", result)\n",
    "\n",
    "        #appends claps to data list\n",
    "        if len(result) == 0:\n",
    "            data.append('0')\n",
    "\n",
    "        else:\n",
    "            data.append(result[0])\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #getting all the text data:\n",
    "    try:\n",
    "        \n",
    "        #<h1>Adding a List of the Text Content to Data</h1>\n",
    "\n",
    "        #grabs all paragraphs in the article\n",
    "        textContent = page_content.find_all(\"p\")\n",
    "        textInfo = str(textContent)\n",
    "\n",
    "        #RegEx that cleans paragraph data by removing html tags and extra commas\n",
    "        result = re.sub(re.compile('<.*?>'), '', textInfo)\n",
    "        result = re.split(\"\\., \", result)\n",
    "        result = result[:-1]\n",
    "        data.append(result)\n",
    "    except:\n",
    "        data.append(None)\n",
    "        \n",
    "    #get tag info    \n",
    "    try:\n",
    "        tagInfo = page_content.find_all(\"ul\")\n",
    "        tags = re.sub(re.compile('<.*?>'), '', str(tagInfo))\n",
    "        tags = re.sub(\"]\", \"\", tags)\n",
    "        tags = re.findall('[A-Z][^A-Z]*', tags)\n",
    "        data.append(tags)\n",
    "        #data\n",
    "    except:\n",
    "        data.append(None)\n",
    "    \n",
    "    #append html to dictionary with data\n",
    "    htmls_with_data[i] = data\n",
    "\n",
    "    \n",
    "    #counter\n",
    "    #print(counter)\n",
    "    counter = counter + 1\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
