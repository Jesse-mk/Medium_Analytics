{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping Art Articles from Medium.com\n",
    "Going into the branches of the DOM and getting relevant tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### references:\n",
    "https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the robots.txt\n",
    "to see what you can scrape at: https://medium.com/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sample - looking at a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "\n",
    "#the url topic (root url)\n",
    "url = \"https://medium.com/topic/art\"\n",
    "\n",
    "#requesting the url to get access to the page\n",
    "r = requests.get(url)\n",
    "\n",
    "#parsing in the information\n",
    "soup = BeautifulSoup(r.content, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the article through the developer tool on a Medium article, there are certain tags which are present within the article which are uniform. \n",
    "\n",
    "- < div class= \"n p\"> : all (title and paragraph)\n",
    "- < div class= \"o n\"> : title specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the html in nice format\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of link I want to get\n",
    "'<a href=\"https://psiloveyou.xyz/remembering-the-terrible-cb7ebf24a6da?source=topic_page---------6------------------1\"'\n",
    "\n",
    "#Regex pattern to get html tags (want entire html)\n",
    "pattern = 'href=\"(.{5,100}source=topic_page\\-+\\d\\-+\\d)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/awake-alive-mind/the-tension-of-expectation-57a0696956c9?source=topic_page---------0------------------1',\n",
       " '/awake-alive-mind/the-tension-of-expectation-57a0696956c9?source=topic_page---------0------------------1',\n",
       " '/awake-alive-mind/the-tension-of-expectation-57a0696956c9?source=topic_page---------0------------------1',\n",
       " '/@georgeannsack?source=topic_page---------0------------------1',\n",
       " '/awake-alive-mind?source=topic_page---------0------------------1',\n",
       " '/awake-alive-mind/the-tension-of-expectation-57a0696956c9?source=topic_page---------0------------------1',\n",
       " '/swlh/what-its-like-to-hear-the-world-in-color-and-in-touch-a5f0db8640aa?source=topic_page---------1------------------1',\n",
       " '/swlh/what-its-like-to-hear-the-world-in-color-and-in-touch-a5f0db8640aa?source=topic_page---------1------------------1',\n",
       " '/swlh/what-its-like-to-hear-the-world-in-color-and-in-touch-a5f0db8640aa?source=topic_page---------1------------------1',\n",
       " '/@juliettefgreene?source=topic_page---------1------------------1']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find all the htmls in the string using REGEX\n",
    "result = re.findall(pattern, str(soup))\n",
    "result[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for link in soup.find_all(\"a\", href = True):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reference: \n",
    "1. https://stackoverflow.com/questions/21006940/how-to-load-all-entries-in-an-infinite-scroll-at-once-to-parse-the-html-in-pytho\n",
    "2. https://stackoverflow.com/questions/37207959/how-to-scrape-all-contents-from-infinite-scroll-website-scrapy\n",
    "3. https://stackoverflow.com/questions/42478591/python-selenium-chrome-webdriver\n",
    "4. https://stackoverflow.com/questions/21006940/how-to-load-all-entries-in-an-infinite-scroll-at-once-to-parse-the-html-in-pytho (used for page scrolling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using Selenium to Scroll and Get all the URLs at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path = r\"C:\\Users\\jesse\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "browser.get(\"https://medium.com/topic/art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code based off of stackoverflow code #4\n",
    "time.sleep(1)\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "pagedowns = 500\n",
    "while pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.5)\n",
    "    pagedowns-=1\n",
    "    \n",
    "#pages = browser.find_elements_by_class_nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the html under the page\n",
    "pages = browser.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex pattern to get html tags\n",
    "pattern = 'href=\"(.{5,100}source=topic_page\\-+\\d{1,5}\\-+\\d)'\n",
    "\n",
    "#get all the html links\n",
    "result = re.findall(pattern, pages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put into a series to process\n",
    "html_links = pd.Series(result)\n",
    "\n",
    "#export to csv for safekeeping\n",
    "html_links.to_csv(\"htmls_art.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the duplicates\n",
    "htmls = html_links.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    /awake-alive-mind/the-tension-of-expectation-5...\n",
       "1    /@georgeannsack?source=topic_page---------0---...\n",
       "2    /awake-alive-mind?source=topic_page---------0-...\n",
       "3    /swlh/what-its-like-to-hear-the-world-in-color...\n",
       "4    /@juliettefgreene?source=topic_page---------1-...\n",
       "dtype: object"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are some repeat html links, so getting rid of the repeats\n",
    "clean_htmls = htmls[~htmls.str.contains(r\"^.{2,30}\\?source=topic\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "3  1\n",
       "6  2\n",
       "8  0\n",
       "9  1"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the topic number in the string (since each one should be \n",
    "#different, to indicate another article)\n",
    "clean_htmls.str.extract(r\"--+(\\d{1,4})-+\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Webscraping Practice on the Infinite Scrolling Pages:\n",
    " refernce: https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016 (also says it's ok to webscrape from this site)\n",
    " \n",
    " Scraping from http://spidyquotes.herokuapp.com/ which is a website to scrape quotes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#the url topic (root url)\n",
    "url = \"http://spidyquotes.herokuapp.com/scroll\"\n",
    "\n",
    "#requesting the url to get access to the page\n",
    "r = requests.get(url)\n",
    "print(r)\n",
    "\n",
    "#parsing in the information\n",
    "soup = BeautifulSoup(r.content, \"lxml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
